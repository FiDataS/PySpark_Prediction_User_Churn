{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering\n",
    "\n",
    "## Load libraries, create Spark session and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import  MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify Project Session Feature Engineering\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/mini_sparkify_event_data.json\"\n",
    "data = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    '''\n",
    "    Function to clean data from userIds and sessionIds from NA and empty values.\n",
    "    \n",
    "    args:\n",
    "        data (pyspark dataframe) - raw data of Sparkify\n",
    "        \n",
    "    returns:\n",
    "        data_clean (pyspark dataframe) - cleaned data of Sparkify\n",
    "    '''\n",
    "    data_clean = data.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "    data_clean = data_clean.filter(data_clean['userId'] != '')\n",
    "\n",
    "    print('The dataset originally contained {} rows. \\nAfter cleaning there are {} rows left.'\n",
    "          .format(data.count(), data_clean.count())) \n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date_function(data_clean):\n",
    "    '''\n",
    "    Function to clean data add columns 'datetime' and 'date' \n",
    "    created from ts column in human readable format\n",
    "    \n",
    "    args:\n",
    "        data_clean (pyspark dataframe) - cleaned data with 'clean_data(data)' function\n",
    "        \n",
    "    returns:\n",
    "        data_transformed (pyspark dataframe) - transformed data with columns 'datetime' and 'date'\n",
    "    '''\n",
    "    udf_convert_ts_to_datetime = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(\n",
    "        timestamp / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    udf_convert_ts_to_date = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(\n",
    "        timestamp / 1000.0).strftime('%Y-%m-%d'))\n",
    "\n",
    "    data_transformed = data_clean.withColumn('datetime', udf_convert_ts_to_datetime(data_clean.ts)) \\\n",
    "        .withColumn(\"date\", udf_convert_ts_to_date(data_clean.ts))\n",
    "    \n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cancellation_event_and_churn_label(data_transformed):\n",
    "    '''\n",
    "    Function to create columns that flag the Churning Event ('Cancellation Confirmation') with 1\n",
    "    and a column that labels entries from users who churn eventually with 1\n",
    "    \n",
    "    args:\n",
    "        data_transformed (pyspark dataframe) - transformed data with 'transform_date_function(data_clean)' function\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new columns 'Churning Event' and 'Churned_User'\n",
    "    '''\n",
    "    #create column that flags Churning Events\n",
    "    data_churn = data_transformed \\\n",
    "        .withColumn('Churning_Event', (F.when(F.col(\"page\")=='Cancellation Confirmation', 1)).otherwise(0))\n",
    "    \n",
    "    #create list of churned users\n",
    "    churned_users = data_churn.select('userId') \\\n",
    "        .filter(data_churn.Churning_Event == 1) \\\n",
    "        .dropDuplicates().collect()\n",
    "\n",
    "    churned_userId = []\n",
    "    for u in churned_users:\n",
    "        churned_userId.append(u[0])\n",
    "    \n",
    "    data_churn = data_churn.withColumn('Churned_User', data_churn.userId.isin(churned_userId))\n",
    "    \n",
    "    total_users = data_churn.select('userId').dropDuplicates().count()\n",
    "    churned_users = data_churn.filter('Churned_User = true').select('userId').dropDuplicates().count()\n",
    "    stayed_users = data_churn.filter('Churned_User = false').select('userId').dropDuplicates().count()\n",
    "    \n",
    "    print('Of total {} users, {} users stayed with the streaming service during the observed time and {} users eventually churned (churn rate: {:2.2f}% ).' \\\n",
    "          .format(total_users, stayed_users, churned_users, churned_users/total_users*100))\n",
    "\n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_platform(x):\n",
    "    \"\"\"\n",
    "    Checks userAgent String for possible platforms that are referenced within the string.\n",
    "    \"\"\"\n",
    "    if 'compatible' in x:\n",
    "        return 'Windows'\n",
    "    elif 'iPad' in x:\n",
    "        return 'iPad'\n",
    "    elif 'iPhone' in x:\n",
    "        return 'iPhone'\n",
    "    elif 'Macintosh' in x:\n",
    "        return 'Mac'\n",
    "    elif 'Windows NT 5.1' in x:\n",
    "        return 'Windows'\n",
    "    elif 'Windows NT 6.0' in x:\n",
    "        return 'Windows'\n",
    "    elif 'Windows NT 6.1' in x:\n",
    "        return 'Windows'\n",
    "    elif 'Windows NT 6.2' in x:\n",
    "        return 'Windows'\n",
    "    elif 'Windows NT 6.3' in x:\n",
    "        return 'Windows'\n",
    "    elif 'X11' in x:\n",
    "        return 'Linux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_browser(x):\n",
    "    '''\n",
    "    Checks userAgent String for possible browsers that are referenced within the string.\n",
    "    '''\n",
    "    if 'Firefox' in x:\n",
    "        return 'Firefox'\n",
    "    elif 'Safari' in x:\n",
    "        if 'Chrome' in x:\n",
    "            return 'Chrome'\n",
    "        else:\n",
    "            return 'Safari'\n",
    "    elif 'Trident' in x:\n",
    "        return 'IE'\n",
    "    else:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_browser_and_platform_columns(data_churn):\n",
    "    '''\n",
    "    Function to create columns for the browser and platform\n",
    "    \n",
    "    args:\n",
    "        data_churn (pyspark dataframe) - data outcome from 'create_cancellation_event_and_churn_label(data_transformed)' function\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new columns 'browser' and 'platform'\n",
    "    '''\n",
    "    # udfs to add columns with the browser and platform\n",
    "    get_browser_udf = F.udf(get_browser, StringType())\n",
    "    get_platform_udf = F.udf(get_platform, StringType())\n",
    "\n",
    "    data_churn = data_churn.withColumn( 'browser', get_browser_udf(data_churn.userAgent) )\n",
    "    data_churn = data_churn.withColumn( 'platform', get_platform_udf(data_churn.userAgent) )\n",
    "    \n",
    "    #dropping userAgent\n",
    "    data_churn = data_churn.drop('userAgent')\n",
    "    \n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same with location data\n",
    "def get_state(x):\n",
    "    \"\"\"\n",
    "    Splits ccolumn values on a \", \" and retrieves the state entry.\n",
    "    \"\"\"\n",
    "    return x.split(', ')[1]\n",
    "\n",
    "def get_city(x):\n",
    "    \"\"\"\n",
    "    Splits column values on a \", \" and retrieves the city entry.\n",
    "    \"\"\"\n",
    "    return x.split(', ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_and_city_columns(data_churn):\n",
    "    '''\n",
    "    Function to create columns for the state and the city\n",
    "    \n",
    "    args:\n",
    "        data_churn (pyspark dataframe) - data outcome from 'create_browser_and_platform_columns(data_churn)' function\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new columns 'state' and 'city'\n",
    "    '''\n",
    "    # udfs to add columns with the state and city\n",
    "    \n",
    "    get_state_udf = F.udf(get_state, StringType())\n",
    "    get_city_udf = F.udf(get_city, StringType())\n",
    "\n",
    "    data_churn = data_churn.withColumn( 'state', get_state_udf(data_churn.location) )\n",
    "    data_churn = data_churn.withColumn( 'city', get_city_udf(data_churn.location) )\n",
    "    \n",
    "    #dropping location\n",
    "    data_churn = data_churn.drop('location')\n",
    "    \n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_days_since_registration_and_percentage_active_days(data_churn):\n",
    "    '''\n",
    "    Function to create a column that shows the days from registration until last recorded event and\n",
    "    create a column that shows the percentage of days where the user was active through the days from registration \n",
    "    until last recorded event\n",
    "    \n",
    "    args:\n",
    "        data_churn (pyspark dataframe) - data outcome from 'create_state_and_city_columns(data_churn)' function\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new column 'days_from_registration_until_last_event' and\n",
    "                                            'percentage_active_days'\n",
    "    '''\n",
    "    \n",
    "    #Create a str datetime column for registration timestamp\n",
    "    udf_convert_ts = F.udf(lambda timestamp: datetime.datetime.fromtimestamp(timestamp / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    data_churn = data_churn.withColumn('dt_registration', udf_convert_ts(data_churn.registration))\n",
    "    \n",
    "    #Create an int column for timedifference from the event to the registration\n",
    "    data_churn = data_churn.withColumn('milliseconds_since_registration', (data_churn.ts - data_churn.registration))\n",
    "    data_churn = data_churn.withColumn('days_since_registration', (data_churn.ts - data_churn.registration)/(1000*60*60*24))\n",
    "    \n",
    "    #Create Column that shows for each user the difference of days for the last event of the user from the registration \n",
    "    #for the observed timeframe\n",
    "    window = Window.partitionBy('userId')\n",
    "    data_churn = data_churn.withColumn('days_from_registration_until_last_event', F.max(data_churn.days_since_registration).over(window))\n",
    "    data_churn = data_churn.withColumn('milliseconds_from_registration_until_last_event', F.max(data_churn.milliseconds_since_registration).over(window))\n",
    "    \n",
    "    data_churn = data_churn.drop('days_since_registration')\n",
    "    \n",
    "    join_df = data_churn.groupBy('userId') \\\n",
    "        .agg(F.countDistinct('date').alias('days_user_active'))\n",
    "    \n",
    "    data_churn = data_churn.join(join_df, on=['userId'], how='full')\n",
    "    \n",
    "    join_df.unpersist(blocking = True)\n",
    "    \n",
    "    data_churn = data_churn \\\n",
    "        .withColumn('percentage_active_days', 100*(data_churn.days_user_active/data_churn.days_from_registration_until_last_event))\n",
    "    \n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_page_events_vs_songs_listened_per_userid(data_churn, list_page_events):\n",
    "    '''\n",
    "    Function to create a column that shows chosen page events (from the list_page_events) vs songs listened to per userId\n",
    "    \n",
    "    args:\n",
    "        data_churn (pyspark dataframe) - data outcome from 'create_column_days_since_registration_and_percentage_active_days(data_churn)' function\n",
    "        list_page_events (list containing strings) - list of strings where each string represents the name of a page event\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new columns for each input event_page\n",
    "    '''\n",
    "    \n",
    "    help_df = data_churn.select(['userId', 'page', 'Churned_User']) \\\n",
    "        .where((data_churn.page.isin(list_page_events)) | (data_churn.page == 'NextSong')) \\\n",
    "        .groupby('userId', 'page', 'Churned_User').count()\n",
    "    \n",
    "    join_df_NextSong = help_df.where(help_df.page == 'NextSong') \\\n",
    "        .select(['userId', 'count']) \\\n",
    "        .withColumnRenamed('count', 'Total_NextSong_perUser')\n",
    "    \n",
    "    data_churn = data_churn.join(join_df_NextSong, on=['userId'], how='full')\n",
    "    \n",
    "    \n",
    "    for page_event in list_page_events:\n",
    "        \n",
    "        page_name = page_event.replace(\" \", \"_\")\n",
    "    \n",
    "        renamed_Column = 'Total_' + page_name + '_perUser'\n",
    "    \n",
    "        join_df_event = help_df.where(help_df.page == page_event) \\\n",
    "            .select(['userId', 'count']) \\\n",
    "            .withColumnRenamed('count', renamed_Column)\n",
    "    \n",
    "        data_churn = data_churn.join(join_df_event, on=['userId'], how='full')\n",
    "    \n",
    "        data_churn = data_churn.withColumn(page_name + '_vs_NextSong', (F.col(renamed_Column)/data_churn.Total_NextSong_perUser))\n",
    "        data_churn = data_churn.drop(renamed_Column)\n",
    "        \n",
    "        help_df.unpersist(blocking = True)\n",
    "        join_df_event.unpersist(blocking = True)\n",
    "        \n",
    "    join_df_NextSong.unpersist(blocking = True)\n",
    "    \n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_streamingtime_per_active_days(data_churn):\n",
    "    '''\n",
    "    Function to add a column to a dataframe that shows the hours of streaming time per active days\n",
    "    \n",
    "    args:\n",
    "        data_churn (pyspark dataframe) - data outcome from 'create_column_page_events_vs_songs_listened_per_userid(data_churn, list_page_events)' function\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new column 'hours_streaming_per_active_day'\n",
    "    '''\n",
    "    streaming_time_df = data_churn.filter(data_churn.page == 'NextSong') \\\n",
    "        .groupBy('userId', 'Churned_User') \\\n",
    "        .agg((F.sum('length')/3600).alias('streamingTime_h')) #'length' column is in seconds\n",
    "    \n",
    "    data_churn = data_churn.join(streaming_time_df.select(['userId','streamingTime_h']), \n",
    "                                 on=['userId'], how='full')\n",
    "        \n",
    "    streaming_time_df.unpersist(blocking = True)\n",
    "    \n",
    "    data_churn = data_churn \\\n",
    "        .withColumn('hours_streaming_per_active_day', data_churn.streamingTime_h/data_churn.days_user_active)\n",
    "    \n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_average_amount_songs_per_session_for_every_user(data_churn):\n",
    "    '''\n",
    "    Function to add a column to a dataframe that shows the average amount of songs played per session per user\n",
    "    \n",
    "    args:\n",
    "        data_churn (pyspark dataframe) - data outcome from 'create_column_streamingtime_per_active_days(data_churn)' function\n",
    "        \n",
    "    returns:\n",
    "        data_churn (pyspark dataframe) - dataframe with new column 'avg_amount_songs_played_per_session'\n",
    "    '''\n",
    "    avg_amount_songs_played_per_session = data_churn.select('userId', 'sessionId', 'Churned_User', 'page') \\\n",
    "        .filter('page = \"NextSong\"') \\\n",
    "        .groupby('userId', 'sessionId', 'Churned_User').count() \\\n",
    "        .select('userId', 'Churned_User', 'count') \\\n",
    "        .groupby('userId', 'Churned_User').mean() \\\n",
    "        .withColumnRenamed('avg(count)', 'avg_amount_songs_played_per_session')\n",
    "    \n",
    "    avg_songs_df = avg_amount_songs_played_per_session.select(['userId', 'avg_amount_songs_played_per_session'])\n",
    "    \n",
    "    data_churn = data_churn.join(avg_songs_df, on=['userId'], how='full')\n",
    "    \n",
    "    avg_amount_songs_played_per_session.unpersist(blocking = True)\n",
    "    avg_songs_df.unpersist(blocking = True)\n",
    "    \n",
    "    return data_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_userIds(raw_feature_data):\n",
    "    '''\n",
    "    Function to convert raw_feature_data to dataframe with one line per userId.\n",
    "    The last recorded event for each user is chosen to get the 'level' column as a feature for the last event\n",
    "    \n",
    "    args:\n",
    "        raw_feature_data (pyspark dataframe) - raw feature dataframe from 'create_column_average_amount_songs_per_session_for_every_user(data_churn)' function\n",
    "        \n",
    "    returns:\n",
    "        formatted_feature_data (pyspark dataframe) - dataframe with userIds and features, one row per userId\n",
    "    '''\n",
    "    \n",
    "    formatted_feature_data = raw_feature_data.where(raw_feature_data.milliseconds_since_registration == \n",
    "                                                    raw_feature_data.milliseconds_from_registration_until_last_event)\n",
    "    \n",
    "    formatted_feature_data = formatted_feature_data.dropDuplicates(subset=['userId', 'ts']) \\\n",
    "                            .fillna(value=0)\n",
    "    \n",
    "    raw_feature_data.unpersist(blocking = True)\n",
    "    \n",
    "    return formatted_feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_dataframe(data):\n",
    "    '''\n",
    "    Function to compute all functions in order and produce the formatted feature dataframe with one row per userId\n",
    "    \n",
    "    args:\n",
    "        data (pyspark dataframe) - unprepared (original) data from Sparkify\n",
    "        \n",
    "    returns:\n",
    "        user_feature_data (pyspark dataframe) - dataframe with userIds and features, one row per userId\n",
    "    '''\n",
    "    #remove columns that are not needed for feature engineering: artist, method, song\n",
    "    data = data.drop('artist', 'method', 'song', 'auth', 'firstName', 'lastName')\n",
    "    \n",
    "    data_clean = clean_data(data)\n",
    "    data.unpersist(blocking = True)\n",
    "    data_clean = transform_date_function(data_clean)\n",
    "    data_churn = create_cancellation_event_and_churn_label(data_clean)\n",
    "    data_clean.unpersist(blocking = True)\n",
    "    \n",
    "    data_feature = create_browser_and_platform_columns(data_churn)\n",
    "    data_churn.unpersist(blocking = True)\n",
    "    data_feature = create_state_and_city_columns(data_feature)\n",
    "\n",
    "    data_feature = create_column_days_since_registration_and_percentage_active_days(data_feature)\n",
    "    list_page_events = ['Thumbs Up', 'Thumbs Down', 'Downgrade', 'Submit Downgrade', 'Upgrade', 'Submit Upgrade', 'Roll Advert', 'Add to Playlist', 'Add Friend']\n",
    "    data_feature = create_column_page_events_vs_songs_listened_per_userid(data_feature, list_page_events)\n",
    "    data_feature = create_column_streamingtime_per_active_days(data_feature)\n",
    "    data_feature = create_column_average_amount_songs_per_session_for_every_user(data_feature)\n",
    "    \n",
    "    \n",
    "    raw_feature_data = data_feature.select('ts', 'itemInSession', 'milliseconds_since_registration', 'milliseconds_from_registration_until_last_event',\n",
    "                                      'Churned_User', 'userId', 'gender', 'level', 'browser', 'platform', 'state', 'city',\n",
    "                                     'days_from_registration_until_last_event', 'Thumbs_Up_vs_NextSong', 'Thumbs_Down_vs_NextSong',\n",
    "                                     'Downgrade_vs_NextSong', 'Submit_Downgrade_vs_NextSong', 'Upgrade_vs_NextSong',\n",
    "                                     'Submit_Upgrade_vs_NextSong', 'Roll_Advert_vs_NextSong', 'Add_to_Playlist_vs_NextSong',\n",
    "                                     'Add_Friend_vs_NextSong','hours_streaming_per_active_day',\n",
    "                                     'percentage_active_days', 'avg_amount_songs_played_per_session')\n",
    "    data_feature.unpersist(blocking = True)\n",
    "    \n",
    "    user_feature_data = create_labeled_userIds(raw_feature_data)\n",
    "    raw_feature_data.unpersist(blocking = True)\n",
    "    \n",
    "    user_feature_data = user_feature_data.drop('ts', 'itemInSession', 'milliseconds_since_registration', \n",
    "                                               'milliseconds_from_registration_until_last_event', 'days_from_registration_until_last_event')\n",
    "    \n",
    "    return user_feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset originally contained 286500 rows. \n",
      "After cleaning there are 278154 rows left.\n",
      "Of total 225 users, 173 users stayed with the streaming service during the observed time and 52 users eventually churned (churn rate: 23.11% ).\n"
     ]
    }
   ],
   "source": [
    "user_feature_data = feature_engineering_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Churned_User: boolean (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- browser: string (nullable = true)\n",
      " |-- platform: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- Thumbs_Up_vs_NextSong: double (nullable = false)\n",
      " |-- Thumbs_Down_vs_NextSong: double (nullable = false)\n",
      " |-- Downgrade_vs_NextSong: double (nullable = false)\n",
      " |-- Submit_Downgrade_vs_NextSong: double (nullable = false)\n",
      " |-- Upgrade_vs_NextSong: double (nullable = false)\n",
      " |-- Submit_Upgrade_vs_NextSong: double (nullable = false)\n",
      " |-- Roll_Advert_vs_NextSong: double (nullable = false)\n",
      " |-- Add_to_Playlist_vs_NextSong: double (nullable = false)\n",
      " |-- Add_Friend_vs_NextSong: double (nullable = false)\n",
      " |-- hours_streaming_per_active_day: double (nullable = false)\n",
      " |-- percentage_active_days: double (nullable = false)\n",
      " |-- avg_amount_songs_played_per_session: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_feature_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe dataframe to be used in the next notebook\n",
    "#user_feature_data.coalesce(1).write.format('json').save(\"user_feature_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data will now be the basis for the data that our models are being trained on. For the next and finals steps towards training the ML models and evaluating the results see 03_Sparkify_Modelling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
